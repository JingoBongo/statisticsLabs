{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Laboratory work nr. 2 part 2</h1>\n",
    "<p>Scebec Mihai, IS-211M</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the laboratory I need to \n",
    "<ul>\n",
    "<li>a) repeat 8 first steps from part 1 for new csv file;</li>\n",
    "<li>b) repeat that for many other regression models;</li>\n",
    "<li>c) find the best one, tune, try to explain;</li>\n",
    "</ul>\n",
    "<p>I will start with straight copy pasting 8 steps for new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is second part, I need to repeat 8 first steps with new dataset\n",
    "# imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pipelinehelper import PipelineHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
      "0             1000025                5                        1   \n",
      "1             1002945                5                        4   \n",
      "2             1015425                3                        1   \n",
      "3             1016277                6                        8   \n",
      "4             1017023                4                        1   \n",
      "5             1017122                8                       10   \n",
      "6             1018099                1                        1   \n",
      "7             1018561                2                        1   \n",
      "8             1033078                2                        1   \n",
      "9             1033078                4                        2   \n",
      "\n",
      "   Uniformity of Cell Shape  Marginal Adhesion  Single Epithelial Cell Size  \\\n",
      "0                         1                  1                            2   \n",
      "1                         4                  5                            7   \n",
      "2                         1                  1                            2   \n",
      "3                         8                  1                            3   \n",
      "4                         1                  3                            2   \n",
      "5                        10                  8                            7   \n",
      "6                         1                  1                            2   \n",
      "7                         2                  1                            2   \n",
      "8                         1                  1                            2   \n",
      "9                         1                  1                            2   \n",
      "\n",
      "   Bare Nuclei  Bland Chromatin  Normal Nucleoli  Mitoses  Class  \n",
      "0            1                3                1        1      2  \n",
      "1           10                3                2        1      2  \n",
      "2            2                3                1        1      2  \n",
      "3            4                3                7        1      2  \n",
      "4            1                3                1        1      2  \n",
      "5           10                9                7        1      4  \n",
      "6           10                3                1        1      2  \n",
      "7            1                3                1        1      2  \n",
      "8            1                1                1        5      2  \n",
      "9            1                2                1        1      2  \n"
     ]
    }
   ],
   "source": [
    "# get dataset\n",
    "dataset = pd.read_csv('Data1.csv')\n",
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I got a wise advice to actually START using np arrays instead of dataframes.\n",
    "X = np.array(dataset.drop('Class', axis= 1))\n",
    "y = np.array(dataset['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another advice I got is to train and fit the model before splitting it into train and test variables\n",
    "standard_scaler = StandardScaler()\n",
    "scaled_matrix = standard_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into tran and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_matrix, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 2, 2, 2,\n",
       "       2, 2, 4, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2, 2, 4, 4, 4, 2, 2, 4, 2, 2,\n",
       "       2, 2, 4, 2, 2, 4, 4, 2, 4, 4, 2, 4, 2, 2, 2, 4, 2, 4, 2, 2, 4, 4,\n",
       "       2, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 4, 2, 4, 2, 2,\n",
       "       2, 2, 4, 4, 4, 2, 4, 2, 4, 4, 2, 4, 2, 2, 2, 4, 2, 2, 2, 4, 2, 2,\n",
       "       4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 2, 4,\n",
       "       2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create SVC, make a prediction\n",
    "svc_classifier = SVC(kernel='linear', random_state = 0)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "svc_prediction = svc_classifier.predict(X_test)\n",
    "svc_prediction\n",
    "# well, not exactly the way to see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[92  0]\n",
      " [ 0 45]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00        92\n",
      "           4       1.00      1.00      1.00        45\n",
      "\n",
      "    accuracy                           1.00       137\n",
      "   macro avg       1.00      1.00      1.00       137\n",
      "weighted avg       1.00      1.00      1.00       137\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# compare prediction with actual results\n",
    "print(confusion_matrix(y_test,svc_prediction))\n",
    "print(classification_report(y_test,svc_prediction))\n",
    "# results differ from one predict to another, but precision in bigger than recall most of the times\n",
    "# and in general looks good enough\n",
    "# \n",
    "# Also this is worth mentioning that without 'fit and transform' function before splitting data into\n",
    "# train and test I was getting worse results than here\n",
    "print(accuracy_score(y_test, svc_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92753623 0.97101449 0.95652174 0.94117647 0.98529412 0.97058824\n",
      " 0.95588235 1.         0.97058824 0.98529412]\n",
      "0.9663895993179882\n"
     ]
    }
   ],
   "source": [
    "# now we will cross validate the result to see if we can reach more accuracy\n",
    "scores = cross_val_score(svc_classifier, scaled_matrix, y, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())\n",
    "# Ugh, it seems we were just very lucky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It already looks that this dataset is much more fitting for model training and making predictions, but let's try other models too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set: {'C': 0.75, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.956 (+/-0.056) for {'C': 0.25, 'kernel': 'linear'}\n",
      "0.956 (+/-0.035) for {'C': 0.25, 'kernel': 'rbf'}\n",
      "0.956 (+/-0.052) for {'C': 0.5, 'kernel': 'linear'}\n",
      "0.956 (+/-0.042) for {'C': 0.5, 'kernel': 'rbf'}\n",
      "0.956 (+/-0.052) for {'C': 0.75, 'kernel': 'linear'}\n",
      "0.960 (+/-0.041) for {'C': 0.75, 'kernel': 'rbf'}\n",
      "0.956 (+/-0.052) for {'C': 1, 'kernel': 'linear'}\n",
      "0.958 (+/-0.042) for {'C': 1, 'kernel': 'rbf'}\n",
      "\n",
      "Best results found with 0.960 mean and parameters {'C': 0.75, 'kernel': 'rbf'}\n",
      "classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00        92\n",
      "           4       1.00      1.00      1.00        45\n",
      "\n",
      "    accuracy                           1.00       137\n",
      "   macro avg       1.00      1.00      1.00       137\n",
      "weighted avg       1.00      1.00      1.00       137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# next step is grid search\n",
    "# set C parameters and kernel\n",
    "param_grid = [\n",
    "  {'C': [0.25, 0.5, 0.75, 1], 'kernel': ['linear','rbf']}\n",
    " ]\n",
    "\n",
    "grid_search_result = GridSearchCV(svc_classifier, param_grid, scoring=\"accuracy\", n_jobs= -2)\n",
    "grid_search_result.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best parameters set found on development set: \"+str(grid_search_result.best_params_))\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = grid_search_result.cv_results_[\"mean_test_score\"]\n",
    "stds = grid_search_result.cv_results_[\"std_test_score\"]\n",
    "best_result = {}\n",
    "mean_zero = 0\n",
    "# zip to loop through several lists\n",
    "for mean, std, params in zip(means, stds, grid_search_result.cv_results_[\"params\"]):\n",
    "    if(mean_zero < mean):\n",
    "        mean_zero = mean\n",
    "        best_result['mean'] = mean\n",
    "        best_result['params'] = params\n",
    "    print(\"%0.3f (+/-%0.03f) for %s\" % (mean, std * 2, params))\n",
    "print()\n",
    "print('Best results found with %0.3f mean and parameters %s' % (best_result['mean'], best_result['params']))\n",
    "# to be fair, all 3 rbf kernels showed the same result\n",
    "print(\"classification report:\")\n",
    "print()\n",
    "y_true, y_pred = y_test, grid_search_result.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy looks almost the same as what we attempted before, so there is nothing to be surprised about, I guess. It is in general outstanding, no matter what parameters we use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Now I need to implement a lot of different models</br>\n",
    "1. Logistic\tRegression\n",
    "2. Decision\tTree Classifier\n",
    "3. Random\tForest\tClassifier\t(with\tnb_trees\t=\t10)\n",
    "4. K- Nearest\tNeighbors (K-NN)\n",
    "5. Naïve\tBayes\n",
    "6. Support\tVector\tMachine\t(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97 accuracy with std of 0.02\n"
     ]
    }
   ],
   "source": [
    "# 1. Logistic regression\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "prediction = logistic_regression.predict(X_test)\n",
    "scores = cross_val_score(logistic_regression, scaled_matrix, y, cv=10)\n",
    "print(\"%0.2f accuracy with std of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making regression models one by one like what you see above is cute, but I want to make the code optimized, so here is my generic approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression  : confusion matrix \n",
      "[[92  0]\n",
      " [ 1 44]]\n",
      "LogisticRegression  : classification report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.99      1.00      0.99        92\n",
      "           4       1.00      0.98      0.99        45\n",
      "\n",
      "    accuracy                           0.99       137\n",
      "   macro avg       0.99      0.99      0.99       137\n",
      "weighted avg       0.99      0.99      0.99       137\n",
      "\n",
      "LogisticRegression  : 0.966 accuracy with std of 0.023\n",
      "\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "\n",
      "DecisionTreeClassifier  : confusion matrix \n",
      "[[91  1]\n",
      " [ 6 39]]\n",
      "DecisionTreeClassifier  : classification report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.94      0.99      0.96        92\n",
      "           4       0.97      0.87      0.92        45\n",
      "\n",
      "    accuracy                           0.95       137\n",
      "   macro avg       0.96      0.93      0.94       137\n",
      "weighted avg       0.95      0.95      0.95       137\n",
      "\n",
      "DecisionTreeClassifier  : 0.947 accuracy with std of 0.026\n",
      "\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "\n",
      "RandomForrestClassifier  : confusion matrix \n",
      "[[92  0]\n",
      " [ 3 42]]\n",
      "RandomForrestClassifier  : classification report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.97      1.00      0.98        92\n",
      "           4       1.00      0.93      0.97        45\n",
      "\n",
      "    accuracy                           0.98       137\n",
      "   macro avg       0.98      0.97      0.97       137\n",
      "weighted avg       0.98      0.98      0.98       137\n",
      "\n",
      "RandomForrestClassifier  : 0.961 accuracy with std of 0.023\n",
      "\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "\n",
      "K-NearestNeighbors  : confusion matrix \n",
      "[[92  0]\n",
      " [ 1 44]]\n",
      "K-NearestNeighbors  : classification report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.99      1.00      0.99        92\n",
      "           4       1.00      0.98      0.99        45\n",
      "\n",
      "    accuracy                           0.99       137\n",
      "   macro avg       0.99      0.99      0.99       137\n",
      "weighted avg       0.99      0.99      0.99       137\n",
      "\n",
      "K-NearestNeighbors  : 0.966 accuracy with std of 0.024\n",
      "\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "\n",
      "NaiveBytes  : confusion matrix \n",
      "[[91  1]\n",
      " [ 1 44]]\n",
      "NaiveBytes  : classification report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.99      0.99      0.99        92\n",
      "           4       0.98      0.98      0.98        45\n",
      "\n",
      "    accuracy                           0.99       137\n",
      "   macro avg       0.98      0.98      0.98       137\n",
      "weighted avg       0.99      0.99      0.99       137\n",
      "\n",
      "NaiveBytes  : 0.962 accuracy with std of 0.020\n",
      "\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "\n",
      "SupportVectorMachine  : confusion matrix \n",
      "[[92  0]\n",
      " [ 0 45]]\n",
      "SupportVectorMachine  : classification report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00        92\n",
      "           4       1.00      1.00      1.00        45\n",
      "\n",
      "    accuracy                           1.00       137\n",
      "   macro avg       1.00      1.00      1.00       137\n",
      "weighted avg       1.00      1.00      1.00       137\n",
      "\n",
      "SupportVectorMachine  : 0.966 accuracy with std of 0.020\n",
      "\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "\n",
      "Best regression model is LogisticRegression with accuracy 0.966 and std 0.023\n"
     ]
    }
   ],
   "source": [
    "best_result = {}\n",
    "mean_zero = 0\n",
    "def generic_regression_model_execution(regression_model, regression_model_name):\n",
    "    global mean_zero\n",
    "    global best_result\n",
    "    regression_model.fit(X_train, y_train)\n",
    "    prediction = regression_model.predict(X_test)\n",
    "    print(regression_model_name,\" : confusion matrix \")\n",
    "    print(confusion_matrix(y_test, prediction))\n",
    "    print(regression_model_name,\" : classification report \")\n",
    "    print(classification_report(y_test, prediction))\n",
    "    scores = cross_val_score(regression_model, scaled_matrix, y, cv=10)\n",
    "    mean = scores.mean()\n",
    "    std = scores.std()\n",
    "    if(mean_zero < mean):\n",
    "        mean_zero = mean\n",
    "        best_result['regression_model_name'] = regression_model_name\n",
    "        best_result['accuracy'] = mean\n",
    "        best_result['std'] = std\n",
    "    print(regression_model_name,\" : %0.3f accuracy with std of %0.3f\" % (mean, std))\n",
    "    print()\n",
    "    print('=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-')\n",
    "    print()\n",
    "    \n",
    "list_of_regression_models = {}\n",
    "list_of_regression_models['LogisticRegression']=LogisticRegression()\n",
    "list_of_regression_models['DecisionTreeClassifier']=DecisionTreeClassifier()\n",
    "list_of_regression_models['RandomForrestClassifier']=RandomForestClassifier(n_estimators=10)\n",
    "list_of_regression_models['K-NearestNeighbors']=KNeighborsClassifier()\n",
    "list_of_regression_models['NaiveBytes']=GaussianNB()\n",
    "list_of_regression_models['SupportVectorMachine']=SVC()\n",
    "for key, value in list_of_regression_models.items():\n",
    "    generic_regression_model_execution(value, key)\n",
    "print(\"Best regression model is %s with accuracy %0.3f and std %0.3f\" % (best_result['regression_model_name'],\n",
    "                                                                         best_result['accuracy'],\n",
    "                                                                         best_result['std']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I would say my way of choosing the best model is not fully correct. I made my choice based on accuracy and std. But logistic model had at least one wrong result, where the support vector machine had 0 and similar accuracy. With 0.3f it is rounded, but in general logistic model somehow has more accuracy than SVM. In case that I made the decision based on missed predictions, I would definitely pick the SVM one. 0 misses, amazing. \n",
    "<p>And once again, this dataset works for all models very nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n",
      "0.972 accuracy with std 0.018\n",
      "tuned hyperparameters :(best parameters)  {'classifier': LogisticRegression(C=0.012742749857031334, solver='liblinear'), 'classifier__C': 0.012742749857031334, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
      "accuracy : 0.967104377104377\n"
     ]
    }
   ],
   "source": [
    "# My best model is Logistic one, so now is the time to tune it's parameters\n",
    "param_grid = [\n",
    "    {'classifier' : [LogisticRegression()],\n",
    "     'classifier__penalty' : ['l1', 'l2'],\n",
    "    'classifier__C' : np.logspace(-4, 4, 20),\n",
    "    'classifier__solver' : ['liblinear']}\n",
    "]\n",
    "\n",
    "# Create grid search object\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 10, verbose=True, n_jobs=-2)\n",
    "\n",
    "\n",
    "# Fit on data\n",
    "\n",
    "best_clf = clf.fit(X_train, y_train)\n",
    "best_prediction = best_clf.predict(X_test)\n",
    "best_scores = cross_val_score(best_clf.best_estimator_, scaled_matrix, y, cv=10)\n",
    "print(\"%0.3f accuracy with std %0.3f\" % (best_scores.mean(), best_scores.std()))\n",
    "print(\"tuned hyperparameters :(best parameters) \",best_clf.best_params_)\n",
    "print(\"accuracy :\",best_clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I trust the internet, I took all parameters I could find there and pasted them here. With some tweaking grid search decided that logistic model's best accuracy is around 0.96 anyway, so I will not pick any parameters here as important ones. Most of results I saw were almost the same. \n",
    "<p>\n",
    "Moreover, as a conclusion I am surprised that logistic model 'won', I personally thought that SVM will fit the best. To be fair, all the models showed great results, and probably if missed shots mattered more than accuracy I would still pick SVM model. I am not that experienced with machine learning to play with parameters more or make some strong assumptions, so I'll just say that this time the dataset was the most important factor why the results are so good, not the models or my coding skills."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b355ee983f23f4d0b28e90ea45b33537f29c12390fbee61533ba394cb277947"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
